[Mode]
# Set logging level
debug = False

[Glidein]
# Address where the state of the remote queue can be accessed
address = http://glidein-simprod.icecube.wisc.edu:11001/jsonrpc

# Whether or not the state of the remote queue was queried 
# from the server or whether it was transmitted through a 
# a text file
ssh_state = True

# How long to wait before considering the next set of jobs
delay = -1

# Location of the tarball, etc.
loc = $HOME/glidein

# Filename of tarball to be extracted
tarball = glidein.tar.gz

# Filename of the executable
executable = glidein_start.sh

[Cluster]
# User under which the jobs are being submitted
user = $USER 

# OS of the cluster
os = RHEL6 

# Scheduler used by cluster
scheduler = PBS

# Submit command for scheduler
submit_command = qsub

# Maximum number jobs that be in the queue
max_total_jobs = 1500

# Number of jobs that can be submitted per round
limit_per_submit = 150

# Memory per computing core in MB
# Needed to make PBS submission efficient
mem_per_core = 2700

# Maximum disk space available
max_disk = 300000

# Walltime used
walltime_hrs = 14

# Is cvmfs available? True/False
cvmfs = True

# Can we submit only jobs that need CPUs? True/False
cpu_only = False

# Can we submit only jobs that need GPUs? True/False
gpu_only = False

# A list of according to which job requirement the 
# job submission should be prioritized. The position 
# in the list indicates the prioritization. ["memory", "disk"] 
# means jobs with high memory will be 
# submitted before jobs with lower memory requirements, 
# followed by jobs with high disk vs. low disk requirement. 
# Jobs with high memory and disk requirements will be submitted first
# then jobs with high memory and medium disk requirements, and so on 
# and so forth. 
prioritize_jobs = ["memory", "disk"]

# Command needed to determine the number of jobs running
running_cmd = showq -u $USER|grep $USER|wc -l

# Group jobs with the same requirements into a 
# single submission.
# Special note for PBS: 
# The option `group_jobs` cannot be used with PBS.
# gridftp does not like the name of the temporary
# directories generated by PBS. The name has a
# `[]` in it, which gridftp takes offense to. 
# groups_jobs = True

# Certain clusters do have local storage on the node
# needed to create a temporary folder on a network
# filesystem. The jobs needs to clean up after itself
# if this is the case
cleanup = True

# Which directory is the scratch dir and needs to be 
# cleaned up
dir_cleanup = /global/scratch/$USER/iceprod/scratch/


[SubmitFile]
# Filename of the submit file
filename = submit.pbs

# Temporary directory environment variable
local_dir = /global/scratch/briedel/iceprod/scratch/${PBS_JOBID}

# Extra line(s) in PBS header
custom_header = #PBS -A ngw-282-ac

# Custom extra line(s) needed for cluster. 
# custom_body = 

# Custom extra line(s) needed at the end of the job 
# custom_footer = 

# Custom environment variables
[CustomEnv]
TMPDIR = /global/scratch/briedel/iceprod/scratch/${PBS_JOBID}

